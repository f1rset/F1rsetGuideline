{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0066937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61dd20c",
   "metadata": {},
   "source": [
    "# PCA for Linear Algebra Project\n",
    "\n",
    "Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "\n",
    "### Centering data:\n",
    "We need to subtract the mean of each variable from all observations (so that the data has a mean of 0). This is important because PCA is sensitive to the scale of the variables.\n",
    "\n",
    "For each vector $x_i$ of the data:\n",
    "\n",
    "$$x_i^{\\prime}​=x_i​ -  \\overline{x_i}​​$$\n",
    "\n",
    "where:\n",
    "- $\\overline{x_i} = \\frac{\\sum_{i=0}^n{x_i}}{n}$\n",
    "\n",
    "Now combine the data into one matrix:\n",
    "\n",
    "$$X_{cent} = \n",
    "\\begin{pmatrix}\n",
    "-x_1-\\\\\n",
    "-x_2-\\\\\n",
    "...\\\\\n",
    "-x_n-\n",
    "\\end{pmatrix}$$\n",
    "### Finding covariance matrix ($C$):\n",
    "\n",
    "PCA (principal component analysis) looks for the directions (components) along which the data has the most variance. To find these directions, we need to understand how the variables \"variate\" together — that is, how one feature varies relative to another. So we need covariance matrix, which is a square matrix of size $n×n$, where $n$ is the number of features. Each element $cov(x_i,x_j)$ in this matrix shows how much two features $x_i$ and $x_j$ change together.\n",
    "\n",
    "Calculate the matrix:\n",
    "$$C = \\frac{1}{n-1}X_{cent}^TX_{cent}$$\n",
    "where:\n",
    "- $X_{cent}$ $-$ Matrix of data\n",
    "\n",
    "### Eigenvalues and eigenvectors of $C$\n",
    "\n",
    "The eigenvectors correspond to the directions in which the data have the greatest variance, and the eigenvalues determine the \"strength\" (importance) of each of these directions.\n",
    "Find the eigenvalues:\n",
    "$$det(C-\\lambda I) = 0$$\n",
    "$$(C-\\lambda I)v = 0$$\n",
    "where: \n",
    "- $\\lambda$ $-$ eigenvalue\n",
    "- $v$ $-$ eigenvector\n",
    "\n",
    "But for computing used <a href=\"https://en.wikipedia.org/wiki/Power_iteration\">Power Method</a> with Rayleigh quotient. So formulas will be:\n",
    "\n",
    "The power iteration algorithm starts with a vector $b_0$, which may be an approximation to the dominant eigenvector or a random vector. The method is described by the recurrence relation\n",
    "\n",
    "$$b_{k+1} = \\frac{Ab_k}{‖Ab_k‖}$$\n",
    "\n",
    "So, at every iteration, the vector $b_{k}$ is multiplied by the matrix $A$ and normalized. \n",
    "\n",
    "Eigenvalue:\n",
    "\n",
    "$$ \\mu_k = \\frac{b^*_kAb_k}{b^*_kb_k}$$\n",
    "\n",
    "converges to the dominant eigenvalue (with <a href=\"https://en.wikipedia.org/wiki/Rayleigh_quotient\">Rayleigh quotient</a>).\n",
    "\n",
    "**Selecting principal components:**\n",
    "\n",
    "The eigenvectors corresponding to the largest eigenvalues become the principal components. The number of principal components is usually determined based on how much of the total variance you want to retain($n$).\n",
    "So, the transformation matrix will be $V_k$ (matrix of the eigenvectors of the $n$ principal components).\n",
    "\n",
    "### Transforming data\n",
    "\n",
    "The final step is to project the data into the principal component space. This is done by multiplying the data matrix ($X$) by the eigenvector matrix ($V_k$).\n",
    "$$X_{pca} = X_{cent}V_k$$\n",
    "\n",
    "<mark>Additional notes:</mark>\n",
    "- Calculating norm is by $\\sqrt{\\sum^n_{i=1}{x_i^2}}$ where $x_i - $ feature.\n",
    "- Eigenvalues are sorted and eigenvectors corresponding them.\n",
    "- Taking components by $\\frac{\\sum_{i=1}^k\\lambda_i}{\\sum_{i=1}^n\\lambda_i}\\le 0.99$ (function **<span style = \"color: lightblue\">def</span> <span style = \"color: lightgreen\">_choose_components</span>**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerPCA:\n",
    "\n",
    "    def _powerIter(self, A):\n",
    "        n = A.shape[1]\n",
    "        v_k = np.random.rand(n)\n",
    "        while True:\n",
    "            v_k1 = np.dot(A, v_k)\n",
    "            v_k1_norm = self._norm(v_k1.T)\n",
    "            v_k1 = v_k1 / v_k1_norm\n",
    "\n",
    "            if v_k1_norm < 1e-10:\n",
    "                return 0.0, np.zeros(n)\n",
    "\n",
    "            if self._norm(np.abs(v_k1 - v_k)) < 1e-10:\n",
    "                v_k = v_k1\n",
    "                break\n",
    "            v_k = v_k1\n",
    "        eigval = np.dot(np.conj(v_k).T, np.dot(A, v_k)) / np.dot(np.conj(v_k).T, v_k)\n",
    "        v = v_k\n",
    "        return eigval, v_k1\n",
    "\n",
    "    def _eig(self, A, cout: bool = False):\n",
    "        eigenvalues = []\n",
    "        eigenvectors = []\n",
    "        A_copy = A.copy()\n",
    "        n = A_copy.shape[1]\n",
    "\n",
    "        for i in range(n):\n",
    "            eigval, v = self._powerIter(A_copy)\n",
    "\n",
    "            if self._norm(v) < 1e-8 or np.isnan(eigval):\n",
    "                continue\n",
    "\n",
    "            eigenvalues.append(eigval)\n",
    "\n",
    "            v = v / self._norm(v)\n",
    "            for v_prime in eigenvectors:\n",
    "                v -= np.dot(v_prime, v) * v_prime\n",
    "            eigenvectors.append(v)\n",
    "            A_copy = A_copy - eigval * np.outer(v, v)\n",
    "\n",
    "        while len(eigenvalues) < n:\n",
    "            eigenvalues.append(0.0)\n",
    "            eigenvectors.append(np.zeros(n))\n",
    "        \n",
    "        if cout:\n",
    "            print(f\"EigResult(eigenvalues={np.array(eigenvalues)}), eigenvectors={np.array(eigenvectors)}))\")\n",
    "        return np.array(eigenvalues), np.array(eigenvectors).T\n",
    "    \n",
    "    def _norm(self, v):\n",
    "        sum_sq = 0.0\n",
    "        for i in range(len(v)):\n",
    "            sum_sq += v[i] ** 2\n",
    "        return sum_sq ** 0.5\n",
    "    \n",
    "    def _choose_components(self, eigenvalues, threshold=0.99):\n",
    "        total = sum(eigenvalues)\n",
    "        running_sum = 0.0\n",
    "        for i, eigval in enumerate(eigenvalues):\n",
    "            running_sum += eigval\n",
    "            if running_sum / total >= threshold:\n",
    "                return i + 1\n",
    "        return len(eigenvalues)\n",
    "    \n",
    "    \n",
    "    def _centerData(self, X, cout = False):\n",
    "        mu = np.mean(X, axis=0)\n",
    "        X_cent = X - mu\n",
    "        return X_cent\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        centeredData = self._centerData(data)\n",
    "    \n",
    "        C = (1 / (centeredData.shape[0] - 1)) * centeredData.T @ centeredData\n",
    "        eigvals, eigvecs = self._eig(C)\n",
    "    \n",
    "        idx = np.argsort(eigvals)[::-1]\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "    \n",
    "        k = self._choose_components(eigvals)\n",
    "        eigvecs = eigvecs[:, :k]\n",
    "    \n",
    "        return centeredData @ eigvecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd93f6",
   "metadata": {},
   "source": [
    "### All links together:\n",
    "\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Power_iteration\">Power Method</a>\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Rayleigh_quotient\">Rayleigh quotient</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
